飞桨4D混合并行训练使用指南
--------------------------

简介
====

在NLP领域，依托飞桨打造的“语义理解技术与平台文心ERNIE”曾获得过无数殊荣：2020年3月一举拿下SemEval 2020的5项冠军；5月发布语言生成预训练模型ERNIE-GEN，刷新语言生成SOTA；6月发布多模态模型ERNIE-ViL，刷新5项任务纪录，登顶权威榜单VCR；7月亮相2020世界人工智能大会，摘取最高荣誉SAIL奖；11月获得中国人工智能学会优秀科技成果奖。在文心ERNIE这些闪耀成绩的背后，也有飞桨的分布式训练技术的贡献。

对于NLP和CV这类网络结构复杂、参数稠密的模型，飞桨分布式训练技术的集合通信模式可以很好的支持这类模型的训练。该模式没有管理模型参数的中心节点，每个节点都是Worker，负责模型训练同时还需要最新的全局梯度信息。集合通信模式对计算芯片的算力和芯片间的网络互连带宽要求较高，如高性能计算的GPU、芯片之间的高速网络互联NVLINK和InfiniBand等，因此非常适合CV和NLP领域计算密集型训练任务。

原理介绍
=======

当前飞桨集合通信模式已经具备支持文心ERNIE千亿语言模型的训练能力，其Sharding-DP策略更是在近期助力文心ERNIE的多项任务分数刷新GLUE榜单。而这个Sharding-DP策略正是飞桨集合通信模式为了训练ERNIE这样的大规模复杂模型所支持的多种并行策略中的一种。那么飞桨是使用哪些策略成功支持文心ERNIE千亿语言模型训练的呢？这些策略是如何工作的呢？接下来将为大家详细介绍。

ERNIE千亿级模型采用100多层transformer网络结构，计算复杂，训练需要占用T级显存资源，如果想用更少的机器高效训练，必须采取一系列性能优化和显存优化措施。

首先看如何性能优化。我们通过一个公式来看哪些因素可以影响训练速度，在固定的硬件环境下：

.. math::

    总训练速度 ∝ 单卡速度 * 卡数 * 多卡加速比

其中单卡速度由数据读取和计算速度决定；多卡加速比由计算/通信效率决定。显而易见，这三个是关键因素。除了单卡可以使用的算子融合、混合精度之类的基础性能优化策略之外，分布式训练还引入一系列并行策略。并行策略的核心思想是将数据和计算有关的图/算子切分到不同设备上，同时尽可能降低设备间通信所需的代价，合理使用多台设备资源，实现高效的并发调度训练，最大化提升训练速度。常见并行策略有数据并行DP（Data Parallel）、Layer间并行（流水线并行PP，Pipeline Parallel）、Layer内并行（模型并行MP，Model Parallel）。我们从设备资源和计算/通信效率来分析三种策略的优缺点： 

1. 数据并行训练加速比最高，但要求每个设备上都备份一份模型，显存占用比较高。为此我们的改进方案是分组参数切片数据并行策略（具体原理后文介绍），兼容了MP+DP的优势，但缺点是通信量大。
2. 模型并行，通信量比较高，适合在机器内做模型并行。
3. 流水线并行，训练设备容易出现空闲状态，加速效率没有DP高；但能减少通信边界支持更多的层数，适合在机器间使用。

+------------+-----------------------------------------------------------------+-------------------------------------------+
| 策略       | 设备资源                                                        | 计算/通信效率                             |
+============+=================================================================+===========================================+
| 数据并行   | 每个设备上都备份一个模型                                        | 受限于总batch size，不能一直增加，影响收敛|
+------------+-----------------------------------------------------------------+-------------------------------------------+
| 模型并行   | 设备越多，单卡资源占用越少                                      | 通信量大，在机器间通信慢，加速比低        |
+------------+-----------------------------------------------------------------+-------------------------------------------+
| 流水线并行 | 流水线路数越多，单卡显存占用越少，但没减少每一层输出的显存占用  | 减少通信边界支持更多层数、需要大Batch来解决设备空闲问题 |
+------------+-----------------------------------------------------------------+-------------------------------------------+

其次看显存问题，通过下表分析的显存占用来源可以看出，上述的并行策略同样可以很好的应对不同来源的显存占用，更多的层数可以通过流水线并行和分组参数切分策略来解决；某层参数很大可以通过模型并行来解决；其次飞桨还提供一些其它灵活的优化方式，例如每层输出占用的显存，可以通过重计算和offload来解决。

+----------------------+-------------------------------------------------------------------------------------------+
| 显存占用来源         | 解决方案                                                                                  |
+======================+===========================================================================================+
| 更多的层数           | 按层切分到不同卡：流水线并行（PP）；分组参数切片策略（Sharding ）                         |
+----------------------+-------------------------------------------------------------------------------------------+
| 更大batch size       | 梯度累积（GradientMerge）小步快跑，累积模拟，提高计算/通信占比                            |
+----------------------+-------------------------------------------------------------------------------------------+
| 参数/梯度            | 模型并行（MP）                                                                            | 
+----------------------+-------------------------------------------------------------------------------------------+
| Activation           | Recompute选择checkpoint，清除部分前向计算结果，需要的时候基于checkpoint重新计算；Offload将显存中的checkpoint加载到内存中  |
+----------------------+-------------------------------------------------------------------------------------------+
| Optimizer State      | Optimizer state offload：在前向、反向时将优化器中的状态（e.g. adam 中的 moment）加载到host 内存中， 仅在 update 阶段将其载入显存，优化其显存占用周期。|
+----------------------+-------------------------------------------------------------------------------------------+
| 临时buffer：碎片     | 显存管理(tensor lifetime等)                                                               |
+----------------------+-------------------------------------------------------------------------------------------+

综上所述，针对性能优化和显存优化，几种并行策略都有用武之地，但是同时也有各自的局限性，所以如果想高效训练千亿模型，需要这几种策略相互组合，取长补短，发挥各自的优势。

那么如何组合呢？飞桨研发人员首先在单机内使用模型并行和分组参数切片组合的2D策略，这么选择的原因是这两个策略通信量较大，适合使用机器内的卡间通信；然后为了承载千亿规模模型，再叠加流水线并行策略，使用多台机器共同分担；最后为了做到高效，在外层又叠加了数据并行来增加并发数量，提升整体训练速度。这样业内首个4D混合并行策略就诞生了。

下面咱们再来简单介绍下几个并行策略的原理。

模型并行策略这里指的是将某一层网络切成多份，并分给不同的卡并行计算，每张卡仅需要计算部分结果。对于ERNIE中的Transformer网络结构，模型并行就可以对全连接层FC切分，然后通过通信操作合并计算结果。

流水线并行策略支持将模型的不同层放置到不同的设备上，通过多个设备来共同分担显存消耗，实现超大规模模型训练。相邻设备间通过通信链路传输数据。由于各个设备间传输的仅是相邻设备间的输出张量，因此通信量较小，相对来说较为适合机器间通信的场景。

值得注意的是，流水线并行可以说是广义模型并行的一种特例，本文中的模型并行仅指Tensor切分，也就是会出现同一层网络交由不同卡来计算的情况，而流水线并行则是按照网络层的粒度切分。

最后再来看下飞桨的分组参数切片策略，其特色是在使用参数切片方式节省显存的基础上和数据并行策略组合成更加强大的Sharding-DP策略。简而言之，这种组合后的策略拥有很强的灵活性，用户可以根据现有硬件环境情况，自由设定模型参数切分的数量（sharding_degree）和数据并行的路数（dp_degree），仅需要确保sharding_degree × dp_degree =总卡数即可。

举个例子，假设用户有4台单机四卡的机器（共16张卡），训一个16层网络的模型。如果模型参数规模可以使用一台机器承载，则推荐使用dp_degree=4 & sharding_degree=4的方式。这种方式的优势在于只有机器内卡间通信，但是模型最大不能超过单台机器所能承受存储范围。

如果模型大小超过了单台机器，问题也不大，用户可以灵活地选择dp_degree=2 & sharding_degree=8方式。与上一种方式相比，这种方式支持的模型参数规模翻倍。

但是在一些特殊的情况下，如果模型参数规模非常大，半数机器都无法承载，则可以进一步使用dp_degree=1 & sharding_degree=16方式，即将整个模型参数交由全部机器承载，这也是标准的ZeRO-DP方式。这种方式跨机器通信数非常高，对训练速度影响很大。其实Sharding-DP可以说是ZeRO-DP的一种升华，让用户可以使用更加高效方式应对特殊场景之外的绝大部分训练任务。

使用方法
=======

可以通过DistributedStrategy配置使用混合并行训练。

.. code-block:: python

   fleet.init(is_collective=True)
   dist_strategy = paddle.distributed.fleet.DistributedStrategy()
   dist_strategy.sharding = args.use_sharding
   dist_strategy.pipeline = args.num_pp > 1
   dist_strategy.sharding_configs = {"segment_broadcast_MB": 32,
                                     "sharding_degree": args.num_sharding,
                                     "mp_degree": args.num_mp,
                                     "pp_degree": args.num_pp,
                                     "dp_degree":args.num_dp,
                                     "hybrid_dp": False,
                                     "pp_allreduce_in_optimize": False,
                                     "gradient_merge_acc_step": 1,
                                     "optimize_offload": False,
                                     }
   dist_strategy.pipeline_configs = {"schedule_mode": "1F1B",
                                     "micro_batch_size": micro_bsz,
                                     "accumulate_steps": acc_steps,
                                     }


示例代码可参见：`examples/hybrid_parallelism <https://github.com/PaddlePaddle/FleetX/tree/develop/examples/hybrid_parallelism>`_。

